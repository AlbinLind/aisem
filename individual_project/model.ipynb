{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from enum import Enum\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputEnum(Enum):\n",
    "    MISLEADING_OTHER = \"misleadingOther\"\n",
    "    MISLEADING_FACTUAL_ERROR = \"misleadingFactualError\"\n",
    "    MISLEADING_MANIPULATED_MEDIA = \"misleadingManipulatedMedia\"\n",
    "    MISLEADING_OUTDATED_INFORMATION = \"misleadingOutdatedInformation\"\n",
    "    MISLEADING_MISSING_IMPORTANT_CONTEXT = \"misleadingMissingImportantContext\"\n",
    "    MISLEADING_UNVERIFIED_CLAIM_AS_FACT = \"misleadingUnverifiedClaimAsFact\"\n",
    "    MISLEADING_SATIRE = \"misleadingSatire\"\n",
    "    TRUSTWORTHY_SOURCES = \"trustworthySources\"\n",
    "    NOT_MISLEADING_FACTUALLY_CORRECT = \"notMisleadingFactuallyCorrect\"\n",
    "    NOT_MISLEADING_OUTDATED_BUT_NOT_WHEN_WRITTEN = (\n",
    "        \"notMisleadingOutdatedButNotWhenWritten\"\n",
    "    )\n",
    "    NOT_MISLEADING_CLEARLY_SATIRE = \"notMisleadingClearlySatire\"\n",
    "    NOT_MISLEADING_PERSONAL_OPINION = \"notMisleadingPersonalOpinion\"\n",
    "\n",
    "\n",
    "class2id = {cls.value: i for i, cls in enumerate(OutputEnum)}\n",
    "id2class = {i: cls.value for i, cls in enumerate(OutputEnum)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = {\n",
    "    \"train\": \"train.tsv\",\n",
    "    \"test\": \"test.tsv\",\n",
    "    \"validation\": \"validation.tsv\",\n",
    "}\n",
    "dataset = load_dataset(\"csv\", delimiter=\"\\t\", data_files=datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:41<00:00, 1442.46 examples/s]\n",
      "Map: 100%|██████████| 6000/6000 [00:04<00:00, 1475.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-cased\"\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    text = example[\"summary\"]\n",
    "    labels = [float(example[class_]) for class_ in class2id]\n",
    "    example = tokenizer(text, truncation=True)\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n",
    "small_dataset_train = (\n",
    "    dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(60_000))])\n",
    ")\n",
    "small_dataset_test = (\n",
    "    dataset[\"test\"].shuffle(seed=42).select([i for i in list(range(6_000))])\n",
    ")\n",
    "\n",
    "tokenized_train = small_dataset_train.map(preprocess)\n",
    "tokenized_test = small_dataset_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-cased\",\n",
    "    num_labels=len(OutputEnum),\n",
    "    id2label=id2class,\n",
    "    label2id=class2id,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def computer_metrics(eval_pred):\n",
    "    pred, labels = eval_pred\n",
    "    pred = sigmoid(pred)\n",
    "    pred = (pred > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(\n",
    "        predictions=pred, references=labels.astype(int).reshape(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=computer_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   82/37500 00:26 < 3:24:23, 3.05 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/skola/aisem/individual_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/skola/aisem/individual_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3207530975341797,\n",
       " 'eval_accuracy': 0.8833333333333333,\n",
       " 'eval_f1': 0.7033898305084746,\n",
       " 'eval_precision': 0.7614678899082569,\n",
       " 'eval_recall': 0.6535433070866141,\n",
       " 'eval_runtime': 13.8239,\n",
       " 'eval_samples_per_second': 21.702,\n",
       " 'eval_steps_per_second': 1.374,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/albin/skola/aisem/individual_project/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'misleadingOther', 'score': 0.013595220632851124},\n",
       "  {'label': 'misleadingFactualError', 'score': 0.03300686925649643},\n",
       "  {'label': 'misleadingManipulatedMedia', 'score': 0.009008774533867836},\n",
       "  {'label': 'misleadingOutdatedInformation', 'score': 0.010411562398076057},\n",
       "  {'label': 'misleadingMissingImportantContext', 'score': 0.1889786422252655},\n",
       "  {'label': 'misleadingUnverifiedClaimAsFact', 'score': 0.038660287857055664},\n",
       "  {'label': 'misleadingSatire', 'score': 0.010134004056453705},\n",
       "  {'label': 'trustworthySources', 'score': 0.8479096293449402},\n",
       "  {'label': 'notMisleadingFactuallyCorrect', 'score': 0.26700371503829956},\n",
       "  {'label': 'notMisleadingOutdatedButNotWhenWritten',\n",
       "   'score': 0.004100655671209097},\n",
       "  {'label': 'notMisleadingClearlySatire', 'score': 0.01117636077105999},\n",
       "  {'label': 'notMisleadingPersonalOpinion', 'score': 0.008002033457159996}]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "classifier(\n",
    "     \"This is clearly taken out of context. The author did not mean to say that, as you can hear in this previous recording\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
