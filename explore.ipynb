{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SemArt/semart_train.csv\", sep=\"\t\", encoding=\"unicode_escape\")\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"timeframe\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_file', 'description', 'author', 'title', 'technique', 'date',\n",
       "       'type', 'school', 'timeframe'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeframe\n",
       "0751-0800       2\n",
       "0801-0850       5\n",
       "0851-0900       3\n",
       "0951-1000       4\n",
       "1001-1050       1\n",
       "1051-1100      26\n",
       "1101-1150      14\n",
       "1151-1200      21\n",
       "1201-1250      29\n",
       "1251-1300     116\n",
       "1301-1350     670\n",
       "1351-1400     347\n",
       "1401-1450     938\n",
       "1451-1500    2059\n",
       "1501-1550    2576\n",
       "1551-1600    1582\n",
       "1601-1650    3404\n",
       "1651-1700    2088\n",
       "1701-1750    1444\n",
       "1751-1800    1114\n",
       "1801-1850    1056\n",
       "1851-1900    1745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"timeframe\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school\n",
       "Finnish             5\n",
       "Portuguese          5\n",
       "Polish             16\n",
       "Norwegian          18\n",
       "Irish              21\n",
       "Other              22\n",
       "Swedish            23\n",
       "Bohemian           29\n",
       "Catalan            35\n",
       "Greek              37\n",
       "Belgian            45\n",
       "Scottish           46\n",
       "Russian            68\n",
       "Swiss              89\n",
       "Danish             94\n",
       "American          113\n",
       "Hungarian         178\n",
       "Austrian          216\n",
       "Netherlandish     217\n",
       "English           411\n",
       "Spanish           852\n",
       "German           1173\n",
       "Flemish          2016\n",
       "French           2556\n",
       "Dutch            2948\n",
       "Italian          8011\n",
       "dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"school\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "study             57\n",
       "interior         452\n",
       "other            509\n",
       "historical       545\n",
       "still-life       926\n",
       "genre           1630\n",
       "mythological    1862\n",
       "landscape       2490\n",
       "portrait        3292\n",
       "religious       7481\n",
       "dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"type\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "technique_df = df.groupby(\"technique\").size().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "## Setup\n",
    "Setting up configurations such as label encoding and different types of data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DataSetEnum(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    TEST = \"test\"\n",
    "    VALIDATION = \"val\"\n",
    "\n",
    "\n",
    "class OutputEnum(Enum):\n",
    "    TIMEFRAME = \"timeframe\"\n",
    "    SCHOOL = \"school\"\n",
    "    TYPE = \"type\"\n",
    "\n",
    "\n",
    "timeframe_encoder = {\n",
    "    \"0801-0850\": 0,\n",
    "    \"0851-0900\": 0,\n",
    "    \"0751-0800\": 0,\n",
    "    \"0951-1000\": 0,\n",
    "    \"1001-1050\": 0,\n",
    "    \"1051-1100\": 0,\n",
    "    \"1101-1150\": 0,\n",
    "    \"1151-1200\": 0,\n",
    "    \"1201-1250\": 0,\n",
    "    \"1251-1300\": 0,\n",
    "    \"1301-1350\": 0,\n",
    "    \"1351-1400\": 0,\n",
    "    \"1401-1450\": 1,\n",
    "    \"1451-1500\": 1,\n",
    "    \"1501-1550\": 2,\n",
    "    \"1551-1600\": 2,\n",
    "    \"1601-1650\": 3,\n",
    "    \"1651-1700\": 3,\n",
    "    \"1701-1750\": 4,\n",
    "    \"1751-1800\": 4,\n",
    "    \"1801-1850\": 5,\n",
    "    \"1851-1900\": 5,\n",
    "}\n",
    "\n",
    "school_encoder = {\n",
    "    \"Finnish\": 0,\n",
    "    \"Portuguese\": 0,\n",
    "    \"Polish\": 0,\n",
    "    \"Norwegian\": 0,\n",
    "    \"Irish\": 0,\n",
    "    \"Other\": 0,\n",
    "    \"Swedish\": 0,\n",
    "    \"Bohemian\": 0,\n",
    "    \"Catalan\": 0,\n",
    "    \"Greek\": 0,\n",
    "    \"Belgian\": 0,\n",
    "    \"Scottish\": 0,\n",
    "    \"Russian\": 0,\n",
    "    \"Swiss\": 0,\n",
    "    \"Danish\": 0,\n",
    "    \"American\": 0,\n",
    "    \"Hungarian\": 0,\n",
    "    \"Austrian\": 0,\n",
    "    \"Netherlandish\": 0,\n",
    "    \"English\": 0,\n",
    "    \"Spanish\": 1,\n",
    "    \"German\": 2,\n",
    "    \"Flemish\": 3,\n",
    "    \"French\": 4,\n",
    "    \"Dutch\": 5,\n",
    "    \"Italian\": 6,\n",
    "}\n",
    "\n",
    "type_encoder = {\n",
    "    \"study\": 0,\n",
    "    \"interior\": 0,\n",
    "    \"other\": 0,\n",
    "    \"historical\": 0,\n",
    "    \"still-life\": 0,\n",
    "    \"genre\": 1,\n",
    "    \"mythological\": 2,\n",
    "    \"landscape\": 3,\n",
    "    \"portrait\": 4,\n",
    "    \"religious\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "Building the model, we are using a pre-trained CNN model called resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim: int):\n",
    "        super(CNN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.resnet.requires_grad_(False)\n",
    "        self.resnet.fc = nn.Linear(2048, output_dim)\n",
    "        self.resnet.fc.requires_grad_(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.resnet(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "We make use of a data loader that dynamically loads the dataset so that we avoid loading all of the images (training set has over 19k images) into memory. We also define some transformations, that are following how resnet was trained. For the training set we are doing some data augmentation to avoid overfitting while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_set: DataSetEnum,\n",
    "        output: OutputEnum,\n",
    "        encoder: dict[str, int] | None = None,\n",
    "    ):\n",
    "        self.data_set = data_set\n",
    "        self.output = output\n",
    "        self.data = pd.read_csv(\n",
    "            f\"SemArt/semart_{data_set.value}.csv\", sep=\"\t\", encoding=\"unicode_escape\"\n",
    "        )\n",
    "        self.data.columns = self.data.columns.str.lower()\n",
    "        self._base_path = Path() / \"SemArt\" / \"Images\"\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                # Get the values from here: https://pytorch.org/vision/0.18/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50\n",
    "                transforms.Resize(232),\n",
    "                # Data augmentation, randomly crop the image and flip it horizontally\n",
    "                # only for the training set\n",
    "                transforms.RandomCrop(224)\n",
    "                if data_set == DataSetEnum.TRAIN\n",
    "                else transforms.CenterCrop(224),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "                if data_set == DataSetEnum.TRAIN\n",
    "                else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        if not encoder:\n",
    "            print(\"Using standard encoder\")\n",
    "            self.encoder = (\n",
    "                timeframe_encoder\n",
    "                if output == OutputEnum.TIMEFRAME\n",
    "                else school_encoder\n",
    "                if output == OutputEnum.SCHOOL\n",
    "                else type_encoder\n",
    "                if output == OutputEnum.TYPE\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        assert self.encoder is not None, \"Invalid output type\"\n",
    "        self.data = self.data[self.data[self.output.value].isin(self.encoder.keys())]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self._output_dim = len(set(self.encoder.values()))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "        # Set label\n",
    "        label = -1\n",
    "        label = self.encoder[self.data[self.output.value][idx]]  # type: ignore\n",
    "\n",
    "        assert label >= 0, \"No label found, invalid output type\"\n",
    "\n",
    "        # load image\n",
    "        raw_image = Image.open(\n",
    "            str((self._base_path / str(self.data[\"image_file\"][idx])).resolve())\n",
    "        ).convert(\"RGB\")\n",
    "        image: torch.Tensor = self.transform(raw_image)  # type: ignore\n",
    "        image = image.to(device)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "Because the dataset is imbalanced in the classes that we are training on we have to use oversampling, here we create a `WeightedRandomSampler` that create probabilities based on how common a class is, this is only used for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_for_balanced_sampler(dataset: ImageDataset) -> np.ndarray:\n",
    "    output_dim = dataset._output_dim\n",
    "    class_sizes = np.zeros(output_dim)\n",
    "    encoder = dataset.encoder\n",
    "    assert encoder is not None\n",
    "    for key, val in encoder.items():\n",
    "        class_sizes[val] += len(dataset.data[dataset.data[dataset.output.value] == key])\n",
    "\n",
    "    weights = np.zeros(len(dataset))\n",
    "    for idx, row in enumerate(dataset.data.iterrows()):\n",
    "        weights[idx] = len(dataset) / class_sizes[encoder[row[1][dataset.output.value]]]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# Use tensorboard to visualize the training process\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Helper function to plot the class prediction\n",
    "def plot_class_prediction(net, images, labels):\n",
    "    preds = net(images)\n",
    "    probs = F.softmax(preds, dim=1)\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    labels = labels.cpu().numpy()\n",
    "    images = images.cpu()\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    for idx in np.arange(6):\n",
    "        ax = fig.add_subplot(2, 3, idx + 1, xticks=[], yticks=[])\n",
    "        image = images[idx].numpy().transpose(1, 2, 0)\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(\n",
    "            f\"{labels[idx]} -> {probs[idx].argmax()}({probs[idx].max():.2f})\",\n",
    "            color=(\"green\" if probs[idx].argmax() == labels[idx] else \"red\"),\n",
    "        )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "def train(model, train_dataloader, criterion, optimizer, writer, epoch):\n",
    "    size = len(train_dataloader)\n",
    "    model.train()\n",
    "    try:\n",
    "        for batch, (images, labels) in tqdm(\n",
    "            enumerate(train_dataloader),\n",
    "            total=size,\n",
    "            leave=False,\n",
    "            desc=f\"Training {epoch + 1}\",\n",
    "            position=1,\n",
    "        ):\n",
    "            pred = model(images)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if batch % 10 == 0:\n",
    "                # accuracy\n",
    "                accuracy = (pred.argmax(1) == labels).sum().item() / len(labels)\n",
    "                writer.add_scalars(\n",
    "                    \"Train/Loss\", {\"train\": loss.item()}, batch + size * epoch\n",
    "                )\n",
    "                writer.add_scalars(\n",
    "                    \"Train/Accuracy\", {\"train\": accuracy}, batch + size * epoch\n",
    "                )\n",
    "                # print(\n",
    "                #     f\"{batch}/{size} - train loss: {loss.item():.4f} - train accuracy: {accuracy:.4f}\"\n",
    "                # )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted\")\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "\n",
    "# TEST\n",
    "def test(model, test_dataloader, criterion, writer, epoch):\n",
    "    size = len(test_dataloader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        confusion_matrix = np.zeros(\n",
    "            (test_dataloader.dataset._output_dim, test_dataloader.dataset._output_dim)\n",
    "        )\n",
    "        random_index = np.random.randint(0, size)\n",
    "        for batch, (images, labels) in tqdm(\n",
    "            enumerate(test_dataloader),\n",
    "            total=size,\n",
    "            leave=False,\n",
    "            desc=f\"Testing {epoch + 1}\",\n",
    "            position=1,\n",
    "        ):\n",
    "            pred = model(images)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(pred, labels)\n",
    "            accuracy = (pred.argmax(1) == labels).sum().item() / len(labels)\n",
    "            writer.add_scalars(\"Test/Loss\", {\"test\": loss.item()}, batch + size * epoch)\n",
    "            writer.add_scalars(\n",
    "                \"Test/Accuracy\", {\"test\": accuracy}, batch + size * epoch\n",
    "            )\n",
    "            # print(\n",
    "            #     f\"{batch}/{size} - test loss: {loss.item():.4f} - test accuracy: {accuracy:.4f}\"\n",
    "            # )\n",
    "\n",
    "            # confusion matrix\n",
    "            for idx, (p, l) in enumerate(zip(pred.argmax(1), labels)):\n",
    "                confusion_matrix[l.item(), p.item()] += 1\n",
    "\n",
    "            if batch == random_index:\n",
    "                writer.add_figure(\n",
    "                    \"Class Prediction\",\n",
    "                    plot_class_prediction(model, images, labels),\n",
    "                    epoch,\n",
    "                    close=True,\n",
    "                )\n",
    "\n",
    "        # plot confusion matrix\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(confusion_matrix, cmap=\"viridis\")\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        writer.add_figure(\"Confusion Matrix\", fig, epoch, close=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "We calculate some metrics on the validation set and show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_dataset: DataLoader, criterion):\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_dataloader = validation_dataset\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_n, (images, labels) in enumerate(\n",
    "            tqdm(test_dataloader, total=len(test_dataloader), leave=False)\n",
    "        ):\n",
    "            pred = model(images)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(pred, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += (pred.argmax(1) == labels).sum().item() / len(labels)\n",
    "\n",
    "    print(f\"Test loss: {running_loss / len(test_dataloader):.3f}\")\n",
    "    print(f\"Test accuracy: {running_accuracy / len(test_dataloader):.3f}\")\n",
    "\n",
    "    return (\n",
    "        running_accuracy / len(test_dataloader),\n",
    "        running_loss / len(test_dataloader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validation(model_class, encoder: dict[str, int], output: OutputEnum):\n",
    "    train_dataset = ImageDataset(DataSetEnum.TRAIN, output, encoder)\n",
    "    weights = get_weights_for_balanced_sampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        sampler=WeightedRandomSampler(weights, len(weights)),\n",
    "    )\n",
    "    test_dataset = ImageDataset(DataSetEnum.TEST, output, encoder)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    model = model_class(train_dataset._output_dim)\n",
    "    model.to(device)\n",
    "    params = model.parameters()\n",
    "    # Optimizer parameters is same as Adam default in tensorflow \n",
    "    optimizer = torch.optim.Adam(params, eps=1e-07)\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in tqdm(\n",
    "        range(epochs), desc=\"Epochs\", leave=True, total=epochs, position=0\n",
    "    ):\n",
    "        train(model, train_dataloader, criterion, optimizer, writer, epoch)\n",
    "        test(model, test_dataloader, criterion, writer, epoch)\n",
    "\n",
    "    validation_dataset = ImageDataset(\n",
    "        DataSetEnum.VALIDATION, output, encoder\n",
    "    )\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
    "    accuracy, loss = validate(\n",
    "        model, validation_dataloader, criterion\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"model_trained_{output.value}_{test_dataset._output_dim}.pth\",\n",
    "    )\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# School encodings\n",
    "school_encoding_one = {\n",
    "    \"American\": 0,\n",
    "    \"Austrian\": 1,\n",
    "    \"Belgian\": 2,\n",
    "    \"Bohemian\": 3,\n",
    "    \"Danish\": 4,\n",
    "    \"Dutch\": 5,\n",
    "    \"English\": 6,\n",
    "    \"Flemish\": 7,\n",
    "    \"French\": 8,\n",
    "    \"German\": 9,\n",
    "    \"Hungarian\": 10,\n",
    "    \"Irish\": 11,\n",
    "    \"Italian\": 12,\n",
    "    \"Netherlandish\": 13,\n",
    "    \"Other\": 14,\n",
    "    \"Polish\": 15,\n",
    "    \"Portuguese\": 16,\n",
    "    \"Russian\": 17,\n",
    "    \"Scottish\": 18,\n",
    "    \"Spanish\": 19,\n",
    "    \"Swedish\": 20,\n",
    "    \"Swiss\": 21,\n",
    "    \"Catalan\": 22,\n",
    "    \"Finnish\": 23,\n",
    "    \"Norwegian\": 24,\n",
    "    \"Greek\": 25,\n",
    "}\n",
    "\n",
    "school_encoding_two = {\n",
    "    \"American\": 0,\n",
    "    \"Austrian\": 4,\n",
    "    \"Belgian\": 4,\n",
    "    \"Bohemian\": 3,\n",
    "    \"Danish\": 1,\n",
    "    \"Dutch\": 4,\n",
    "    \"English\": 4,\n",
    "    \"Flemish\": 4,\n",
    "    \"French\": 4,\n",
    "    \"German\": 4,\n",
    "    \"Hungarian\": 3,\n",
    "    \"Irish\": 4,\n",
    "    \"Italian\": 2,\n",
    "    \"Netherlandish\": 4,\n",
    "    \"Other\": 5,\n",
    "    \"Polish\": 3,\n",
    "    \"Portuguese\": 2,\n",
    "    \"Russian\": 3,\n",
    "    \"Scottish\": 4,\n",
    "    \"Spanish\": 2,\n",
    "    \"Swedish\": 1,\n",
    "    \"Swiss\": 4,\n",
    "    \"Catalan\": 2,\n",
    "    \"Finnish\": 1,\n",
    "    \"Norwegian\": 1,\n",
    "    \"Greek\": 2,\n",
    "}\n",
    "\n",
    "type_encoding = {\n",
    "    \"genre\": 0,\n",
    "    \"historical\": 1,\n",
    "    \"interior\": 2,\n",
    "    \"landscape\": 3,\n",
    "    \"mythological\": 4,\n",
    "    \"other\": 5,\n",
    "    \"portrait\": 6,\n",
    "    \"religious\": 7,\n",
    "    \"still-life\": 8,\n",
    "    \"study\": 9,\n",
    "}\n",
    "\n",
    "timeframe_encoding_one = {\n",
    "    \"0801-0850\": 0,\n",
    "    \"0851-0900\": 0,\n",
    "    \"0751-0800\": 0,\n",
    "    \"0951-1000\": 0,\n",
    "    \"1001-1050\": 0,\n",
    "    \"1051-1100\": 0,\n",
    "    \"1101-1150\": 0,\n",
    "    \"1151-1200\": 0,\n",
    "    \"1201-1250\": 0,\n",
    "    \"1251-1300\": 0,\n",
    "    \"1301-1350\": 0,\n",
    "    \"1351-1400\": 0,\n",
    "    \"1401-1450\": 1,\n",
    "    \"1451-1500\": 2,\n",
    "    \"1501-1550\": 3,\n",
    "    \"1551-1600\": 4,\n",
    "    \"1601-1650\": 5,\n",
    "    \"1651-1700\": 6,\n",
    "    \"1701-1750\": 7,\n",
    "    \"1751-1800\": 8,\n",
    "    \"1801-1850\": 9,\n",
    "    \"1851-1900\": 10,\n",
    "}\n",
    "\n",
    "timeframe_encoding_two = {\n",
    "    \"0801-0850\": 0,\n",
    "    \"0851-0900\": 0,\n",
    "    \"0751-0800\": 0,\n",
    "    \"0951-1000\": 0,\n",
    "    \"1001-1050\": 0,\n",
    "    \"1051-1100\": 0,\n",
    "    \"1101-1150\": 0,\n",
    "    \"1151-1200\": 0,\n",
    "    \"1201-1250\": 0,\n",
    "    \"1251-1300\": 0,\n",
    "    \"1301-1350\": 0,\n",
    "    \"1351-1400\": 0,\n",
    "    \"1401-1450\": 1,\n",
    "    \"1451-1500\": 1,\n",
    "    \"1501-1550\": 2,\n",
    "    \"1551-1600\": 2,\n",
    "    \"1601-1650\": 3,\n",
    "    \"1651-1700\": 3,\n",
    "    \"1701-1750\": 4,\n",
    "    \"1751-1800\": 4,\n",
    "    \"1801-1850\": 5,\n",
    "    \"1851-1900\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ff6754f04348e69018cceb2cb560d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8871b98d8c4baa97f5a9eac7f96cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 1:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=[\"Accuracy\", \"Loss\"])\n",
    "\n",
    "acc, loss = train_and_validation(CNN, school_encoding_one, OutputEnum.SCHOOL)\n",
    "results.loc[\"School Non-grouped\"] = [acc, loss]\n",
    "\n",
    "acc, loss = train_and_validation(CNN, school_encoding_two, OutputEnum.SCHOOL)\n",
    "results.loc[\"School Grouped\"] = [acc, loss]\n",
    "\n",
    "acc, loss = train_and_validation(CNN, type_encoding, OutputEnum.TYPE)\n",
    "results.loc[\"Type Encoding\"] = [acc, loss]\n",
    "\n",
    "acc, loss = train_and_validation(CNN, timeframe_encoding_one, OutputEnum.TIMEFRAME)\n",
    "results.loc[\"Timeframe Non-grouped\"] = [acc, loss]\n",
    "\n",
    "acc, loss = train_and_validation(CNN, timeframe_encoding_two, OutputEnum.TIMEFRAME)\n",
    "results.loc[\"Timeframe Grouped\"] = [acc, loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results.to_csv(\"results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
